{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhvU7wHPS3l8"
   },
   "source": [
    "## 1. Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-bhA5QTS2Fo"
   },
   "outputs": [],
   "source": [
    "!pip install facenet-pytorch mtcnn scikit-learn numpy pandas opencv-python-headless tqdm torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6AecQV2UUy1"
   },
   "source": [
    "## 2. Mounting Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwXNXSV9ng71",
    "outputId": "4858c031-8d50-4aa8-82c3-d14ac04a64f7"
   },
   "outputs": [],
   "source": [
    "# connect colab to drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Muik4jVUdzF"
   },
   "source": [
    "## 3. Setting up Project Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_i1QfN9nmMG",
    "outputId": "a344d6db-529a-4718-f146-08e928be1a96"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "project_root = '/content/drive/MyDrive/FaceRecognitionProject'\n",
    "dataset_root = os.path.join(project_root, 'dataset')\n",
    "cropped_root = os.path.join(project_root, 'cropped_faces')\n",
    "embeddings_root = os.path.join(project_root, 'embeddings')\n",
    "model_root = os.path.join(project_root, 'model')\n",
    "\n",
    "for path in [dataset_root, cropped_root, embeddings_root, model_root]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "os.makedirs(os.path.join(dataset_root, 'Me'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_root, 'Unknown'), exist_ok=True)\n",
    "\n",
    "print(\"Project folder structure prepared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UHV3NGPUlQT"
   },
   "source": [
    "## 4. Face Cropping and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150,
     "referenced_widgets": [
      "7dce905620b84319beaf14864c5d5217",
      "416096a1cf884381ad53bdcf3c603d7e",
      "35e56242f3264ecd85072a4659d4106d",
      "4fef4f3629ac4be2a2964f635faba1f8",
      "e7643fcecf36480db31fc9886aec19b2",
      "e19d3f79512043fcb6fde621db45a5af",
      "818f347d031c40bfadd300ed3104caea",
      "70b79ffbaa984abf8444a4e98b0b675e",
      "6649687fdc944cc1b776a19e916437da",
      "f76ff94fbaad45c4a4a7b6ee08fa7ea1",
      "d318269424f342e8907e6b07124a5a28",
      "0d2ffca7012c4335932de59f4a3c684c",
      "03b4364460394594b8e998989f6edb51",
      "ff742e580940451d9fabcaba51d7c7af",
      "257ccfe789cf458ba13bc53aef0a9705",
      "706f08bdb17946538355c1d0cfc8737a",
      "8047237d6edd45caa47d55883af2bf28",
      "c4de85504401494298940a4601111e3f",
      "8866347a395b419eab8596bc3afb5713",
      "ac16bca8f0734da5b196391a1c1cf009",
      "d7dabff08db54551a979ce0622784a0b",
      "e0e8d3090b334fd9ac0215f06e92038b"
     ]
    },
    "id": "1l0hMKjynp6j",
    "outputId": "ace6abfc-93a2-4c79-c85f-b7fd508b62b6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Folder paths\n",
    "data_root = '/content/drive/My Drive/FaceRecognitionProject/dataset'\n",
    "cropped_root = '/content/drive/My Drive/FaceRecognitionProject/cropped_faces'\n",
    "\n",
    "# Load MTCNN model and assign to CPU\n",
    "device = torch.device('cpu')\n",
    "mtcnn = MTCNN(keep_all=True).to(device)\n",
    "print(f\"MTCNN model loaded to {device} device.\")\n",
    "\n",
    "# Define transformations for data augmentation\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "])\n",
    "\n",
    "def gather_image_paths(data_root):\n",
    "    paths = []\n",
    "    labels = []\n",
    "    for cls in ['Me', 'Unknown']:\n",
    "        class_dir = os.path.join(data_root, cls)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        for fname in os.listdir(class_dir):\n",
    "            if fname.lower().endswith(('.jpg','.jpeg','.png', '.bmp')):\n",
    "                paths.append(os.path.join(class_dir, fname))\n",
    "                labels.append(cls)\n",
    "    return np.array(paths), np.array(labels)\n",
    "\n",
    "# Gather original photo paths\n",
    "all_paths, all_labels = gather_image_paths(data_root)\n",
    "\n",
    "if len(all_paths) == 0:\n",
    "    print(\"Error: No images found in the dataset folder. Please make sure you have added images.\")\n",
    "else:\n",
    "    # Split original photos with identity-based split\n",
    "    X_train_paths, X_test_paths, y_train_labels, y_test_labels = train_test_split(\n",
    "        all_paths, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n",
    "    )\n",
    "\n",
    "    # Create folders\n",
    "    for split in ['train', 'test']:\n",
    "        for cls in ['Me', 'Unknown']:\n",
    "            os.makedirs(os.path.join(cropped_root, split, cls), exist_ok=True)\n",
    "\n",
    "    def process_and_augment(image_paths, labels, split_dir):\n",
    "        for i in tqdm(range(len(image_paths)), desc=f'Cropping faces and augmenting data ({split_dir})'):\n",
    "            img_path = image_paths[i]\n",
    "            label = labels[i]\n",
    "\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                boxes, _ = mtcnn.detect(img)\n",
    "\n",
    "                if boxes is not None and len(boxes) > 0:\n",
    "                    box = boxes[0]\n",
    "                    face = img.crop(box)\n",
    "\n",
    "                    # Save the original cropped face\n",
    "                    face.save(os.path.join(cropped_root, split_dir, label, f'original_{os.path.basename(img_path)}'))\n",
    "\n",
    "                    # Apply data augmentation only to training data\n",
    "                    if split_dir == 'train':\n",
    "                        for j in range(5):  # 5 augmented versions per image\n",
    "                            augmented_face = data_transforms(face)\n",
    "                            augmented_face.save(os.path.join(cropped_root, split_dir, label, f'augmented_{j}_{os.path.basename(img_path)}'))\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {img_path} - {e}\")\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        print(\"Cropping faces and applying data augmentation for training...\")\n",
    "        process_and_augment(X_train_paths, y_train_labels, 'train')\n",
    "        print(\"Cropping faces for testing...\")\n",
    "        process_and_augment(X_test_paths, y_test_labels, 'test')\n",
    "        print(\"Face cropping and data augmentation completed. You can proceed to the next step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGdaYvIgUrkX"
   },
   "source": [
    "## 5. Generating Face Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wdO8dlZn0dX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Folder paths\n",
    "cropped_root = '/content/drive/My Drive/FaceRecognitionProject/cropped_faces'\n",
    "embeddings_path = '/content/drive/My Drive/FaceRecognitionProject/embeddings'\n",
    "\n",
    "os.makedirs(embeddings_path, exist_ok=True)\n",
    "\n",
    "# Move FaceNet model to GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device used for training: {device}')\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "def get_embeddings_and_labels(data_dir):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    for class_name in ['Me', 'Unknown']:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        for img_name in tqdm(os.listdir(class_dir), desc=f'Generating embeddings for {os.path.basename(data_dir)}...'):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_tensor = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])(img).unsqueeze(0).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    embedding = resnet(img_tensor).detach().cpu().numpy().flatten()\n",
    "                embeddings.append(embedding)\n",
    "                labels.append(class_name)\n",
    "            except Exception as e:\n",
    "                print(f'Error: An issue occurred while processing {img_path}. - {e}')\n",
    "    return np.array(embeddings), np.array(labels)\n",
    "\n",
    "# Process training and test data\n",
    "X_train_embeddings, y_train_labels = get_embeddings_and_labels(os.path.join(cropped_root, 'train'))\n",
    "X_test_embeddings, y_test_labels = get_embeddings_and_labels(os.path.join(cropped_root, 'test'))\n",
    "\n",
    "if len(X_train_embeddings) > 0 and len(X_test_embeddings) > 0:\n",
    "    # Normalization\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_embeddings)\n",
    "    X_test_scaled = scaler.transform(X_test_embeddings)\n",
    "    joblib.dump(scaler, os.path.join(embeddings_path, 'scaler.joblib'))\n",
    "\n",
    "    # Save NumPy files\n",
    "    np.save(os.path.join(embeddings_path, 'X_train.npy'), X_train_scaled)\n",
    "    np.save(os.path.join(embeddings_path, 'y_train.npy'), y_train_labels)\n",
    "    np.save(os.path.join(embeddings_path, 'X_test.npy'), X_test_scaled)\n",
    "    np.save(os.path.join(embeddings_path, 'y_test.npy'), y_test_labels)\n",
    "\n",
    "    print('All embeddings successfully generated and saved to Google Drive.')\n",
    "else:\n",
    "    print(\"Insufficient data found for training or testing. Please check the first step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9YtOSXmU0dp"
   },
   "source": [
    "## 6. Training and Evaluating KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AhUKd_Bwn3Tv",
    "outputId": "a17300df-d56e-490f-e071-9bd95cdfb7d3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Folder paths\n",
    "project_root = '/content/drive/My Drive/FaceRecognitionProject'\n",
    "embeddings_root = os.path.join(project_root, 'embeddings')\n",
    "model_root = os.path.join(project_root, 'model')\n",
    "\n",
    "# Load saved data\n",
    "try:\n",
    "    X_train = np.load(os.path.join(embeddings_root, 'X_train.npy'))\n",
    "    y_train = np.load(os.path.join(embeddings_root, 'y_train.npy'))\n",
    "    X_test = np.load(os.path.join(embeddings_root, 'X_test.npy'))\n",
    "    y_test = np.load(os.path.join(embeddings_root, 'y_test.npy'))\n",
    "except FileNotFoundError:\n",
    "    print(\"Required .npy files not found. Please make sure you have run the previous steps.\")\n",
    "    exit()\n",
    "\n",
    "# Train KNN classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=7)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(knn_classifier, os.path.join(model_root, 'knn_classifier.joblib'))\n",
    "print(\"KNN classifier successfully trained and saved.\")\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Measure performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=['Me', 'Unknown'])\n",
    "class_report = classification_report(y_test, y_pred, target_names=['Me', 'Unknown'])\n",
    "\n",
    "print(f\"Model accuracy: {accuracy:.2%}\\n\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFoChOcYU8hs"
   },
   "source": [
    "## 7. Live Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 775
    },
    "id": "gaVh25e_oKam",
    "outputId": "57dea303-40e2-436d-c4b1-496408b9a5a5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "print(\"Checking required libraries...\")\n",
    "try:\n",
    "    import facenet_pytorch\n",
    "    import mtcnn\n",
    "    import joblib\n",
    "    import cv2\n",
    "    import PIL\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"Libraries successfully loaded.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error: Some required libraries are missing. Please run the 'Step 1: Installing Dependencies' cell again. Details: {e}\")\n",
    "    pass # Allow code to continue to show other potential issues\n",
    "\n",
    "# Mount Google Drive (if session was reset)\n",
    "print(\"\\nConnecting to Google Drive...\")\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Drive connection successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting Google Drive: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- LOAD DEVICE AND MODELS ---\n",
    "device = torch.device('cpu')\n",
    "print(f'Device used for training and inference: {device}')\n",
    "\n",
    "try:\n",
    "    # Load MTCNN and FaceNet models to CPU\n",
    "    mtcnn = MTCNN(keep_all=True).to(device)\n",
    "    resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "    print(\"FaceNet and MTCNN models successfully loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading FaceNet or MTCNN models: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- LOAD TRAINED MODELS ---\n",
    "project_root = '/content/drive/My Drive/FaceRecognitionProject'\n",
    "model_path = os.path.join(project_root, 'model', 'knn_classifier.joblib')\n",
    "scaler_path = os.path.join(project_root, 'embeddings', 'scaler.joblib')\n",
    "\n",
    "def recognize_live_face(knn_classifier_model, scaler_model, mtcnn_detector, resnet_model, device_used):\n",
    "    \"\"\"\n",
    "    Performs face recognition from live webcam feed using loaded models.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print('Camera is opening, please press the \"Capture Image\" button.')\n",
    "        filename = take_photo()\n",
    "\n",
    "        # Read image with OpenCV\n",
    "        img = cv2.imread(filename)\n",
    "        if img is None:\n",
    "             print(\"Error: Could not read image file.\")\n",
    "             return 'Image Read Error'\n",
    "\n",
    "        # Convert to PIL format\n",
    "        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Face detection\n",
    "        boxes, _ = mtcnn_detector.detect(img_pil)\n",
    "\n",
    "        if boxes is None or len(boxes) == 0:\n",
    "            print('No face found in the image.')\n",
    "            return 'No Face Detected'\n",
    "\n",
    "        # Process all detected faces\n",
    "        predictions = []\n",
    "        for box in boxes:\n",
    "            try:\n",
    "                box = box.astype(int)\n",
    "                # Ensure box coordinates are within image bounds\n",
    "                box = [max(0, box[0]), max(0, box[1]), min(img_pil.width, box[2]), min(img_pil.height, box[3])]\n",
    "\n",
    "                face_img = img_pil.crop(box)\n",
    "\n",
    "                # Resize face to expected input size for FaceNet\n",
    "                face_img = face_img.resize((160, 160))\n",
    "\n",
    "                # Convert cropped face to tensor and normalize\n",
    "                face_tensor = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # VGGFace2 normalization\n",
    "                ])(face_img).unsqueeze(0).to(device_used)\n",
    "\n",
    "\n",
    "                # Extract embedding with FaceNet\n",
    "                with torch.no_grad():\n",
    "                    embedding = resnet_model(face_tensor).detach().cpu().numpy().reshape(1, -1)\n",
    "\n",
    "                # Normalize with trained scaler\n",
    "                # Must be normalized in the same way as the data the model was trained on.\n",
    "                scaled_embedding = scaler_model.transform(embedding)\n",
    "\n",
    "                # Make prediction with KNN model\n",
    "                prediction = knn_classifier_model.predict(scaled_embedding)\n",
    "                predictions.append(prediction[0])\n",
    "\n",
    "                # Draw prediction result and box on the image\n",
    "                # Choose a color based on prediction\n",
    "                color = (0, 255, 0) if prediction[0] == 'Me' else (0, 0, 255) # Green for Me, Red for Unknown\n",
    "                cv2.putText(img, prediction[0], (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "                cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing a detected face: {e}\")\n",
    "                predictions.append('Processing Error')\n",
    "                pass\n",
    "\n",
    "        cv2_imshow(img)\n",
    "        print(f'Predictions: {predictions}')\n",
    "        return predictions\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'An overall error occurred during live face recognition: {e}')\n",
    "        return ['Overall Error']\n",
    "\n",
    "# Javascript function to capture image from camera (kept as is)\n",
    "def take_photo(filename='photo.jpg', quality=0.8):\n",
    "    js = Javascript('''\n",
    "        async function takePhoto(quality) {\n",
    "            const div = document.createElement('div');\n",
    "            const capture = document.createElement('button');\n",
    "            capture.textContent = 'Capture Image';\n",
    "            div.appendChild(capture);\n",
    "\n",
    "            const video = document.createElement('video');\n",
    "            video.style.display = 'block';\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "            document.body.appendChild(div);\n",
    "            div.appendChild(video);\n",
    "            video.srcObject = stream;\n",
    "            await video.play();\n",
    "\n",
    "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "            return new Promise((resolve) => {\n",
    "                capture.onclick = () => {\n",
    "                    const canvas = document.createElement('canvas');\n",
    "                    canvas.width = video.videoWidth;\n",
    "                    canvas.height = video.videoHeight;\n",
    "                    canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "                    stream.getVideoTracks()[0].stop();\n",
    "                    div.remove();\n",
    "                    resolve(canvas.toDataURL('image/jpeg', quality));\n",
    "                };\n",
    "            });\n",
    "        }\n",
    "    ''')\n",
    "    display(js)\n",
    "    data = eval_js('takePhoto({})'.format(quality))\n",
    "    binary = b64decode(data.split(',')[1])\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(binary)\n",
    "    return filename\n",
    "\n",
    "\n",
    "# -- CALL THE RECOGNITION FUNCTION ONLY IF MODELS ARE LOADED --\n",
    "try:\n",
    "    print('Attempting to load classifier and scaler...')\n",
    "    knn_classifier = joblib.load(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    print('Classifier and scaler successfully loaded.')\n",
    "\n",
    "    # If loading is successful, proceed with live recognition\n",
    "    print(\"\\nStarting live face recognition...\")\n",
    "    recognize_live_face(knn_classifier, scaler, mtcnn, resnet, device)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f'\\nError: Classifier or scaler file not found. Please ensure you have completed the model training steps ({model_path}, {scaler_path}).')\n",
    "except Exception as e:\n",
    "    print(f'\\nError loading classifier or scaler: {e}')\n",
    "\n",
    "print(\"\\nLive recognition process finished (either completed or loading failed).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 775
    },
    "id": "1jrXI_0GVo46",
    "outputId": "dc2bb268-3728-4b04-a8ca-67dae0215fcc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "print(\"Checking required libraries...\")\n",
    "try:\n",
    "    import facenet_pytorch\n",
    "    import mtcnn\n",
    "    import joblib\n",
    "    import cv2\n",
    "    import PIL\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"Libraries successfully loaded.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error: Some required libraries are missing. Please run the 'Step 1: Installing Dependencies' cell again. Details: {e}\")\n",
    "    pass # Allow code to continue to show other potential issues\n",
    "\n",
    "# Mount Google Drive (if session was reset)\n",
    "print(\"\\nConnecting to Google Drive...\")\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Drive connection successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting Google Drive: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- LOAD DEVICE AND MODELS ---\n",
    "device = torch.device('cpu')\n",
    "print(f'Device used for training and inference: {device}')\n",
    "\n",
    "try:\n",
    "    # Load MTCNN and FaceNet models to CPU\n",
    "    mtcnn = MTCNN(keep_all=True).to(device)\n",
    "    resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "    print(\"FaceNet and MTCNN models successfully loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading FaceNet or MTCNN models: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- LOAD TRAINED MODELS ---\n",
    "project_root = '/content/drive/My Drive/FaceRecognitionProject'\n",
    "model_path = os.path.join(project_root, 'model', 'knn_classifier.joblib')\n",
    "scaler_path = os.path.join(project_root, 'embeddings', 'scaler.joblib')\n",
    "\n",
    "def recognize_live_face(knn_classifier_model, scaler_model, mtcnn_detector, resnet_model, device_used):\n",
    "    \"\"\"\n",
    "    Performs face recognition from live webcam feed using loaded models.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print('Camera is opening, please press the \"Capture Image\" button.')\n",
    "        filename = take_photo()\n",
    "\n",
    "        # Read image with OpenCV\n",
    "        img = cv2.imread(filename)\n",
    "        if img is None:\n",
    "             print(\"Error: Could not read image file.\")\n",
    "             return 'Image Read Error'\n",
    "\n",
    "        # Convert to PIL format\n",
    "        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Face detection\n",
    "        boxes, _ = mtcnn_detector.detect(img_pil)\n",
    "\n",
    "        if boxes is None or len(boxes) == 0:\n",
    "            print('No face found in the image.')\n",
    "            return 'No Face Detected'\n",
    "\n",
    "        # Process all detected faces\n",
    "        predictions = []\n",
    "        for box in boxes:\n",
    "            try:\n",
    "                box = box.astype(int)\n",
    "                # Ensure box coordinates are within image bounds\n",
    "                box = [max(0, box[0]), max(0, box[1]), min(img_pil.width, box[2]), min(img_pil.height, box[3])]\n",
    "\n",
    "                face_img = img_pil.crop(box)\n",
    "\n",
    "                # Resize face to expected input size for FaceNet\n",
    "                face_img = face_img.resize((160, 160))\n",
    "\n",
    "                # Convert cropped face to tensor and normalize\n",
    "                face_tensor = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # VGGFace2 normalization\n",
    "                ])(face_img).unsqueeze(0).to(device_used)\n",
    "\n",
    "\n",
    "                # Extract embedding with FaceNet\n",
    "                with torch.no_grad():\n",
    "                    embedding = resnet_model(face_tensor).detach().cpu().numpy().reshape(1, -1)\n",
    "\n",
    "                # Normalize with trained scaler\n",
    "                # Must be normalized in the same way as the data the model was trained on.\n",
    "                scaled_embedding = scaler_model.transform(embedding)\n",
    "\n",
    "                # Make prediction with KNN model\n",
    "                prediction = knn_classifier_model.predict(scaled_embedding)\n",
    "                predictions.append(prediction[0])\n",
    "\n",
    "                # Draw prediction result and box on the image\n",
    "                # Choose a color based on prediction\n",
    "                color = (0, 255, 0) if prediction[0] == 'Me' else (0, 0, 255) # Green for Me, Red for Unknown\n",
    "                cv2.putText(img, prediction[0], (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "                cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing a detected face: {e}\")\n",
    "                predictions.append('Processing Error')\n",
    "                pass\n",
    "\n",
    "        cv2_imshow(img)\n",
    "        print(f'Predictions: {predictions}')\n",
    "        return predictions\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'An overall error occurred during live face recognition: {e}')\n",
    "        return ['Overall Error']\n",
    "\n",
    "# Javascript function to capture image from camera (kept as is)\n",
    "def take_photo(filename='photo.jpg', quality=0.8):\n",
    "    js = Javascript('''\n",
    "        async function takePhoto(quality) {\n",
    "            const div = document.createElement('div');\n",
    "            const capture = document.createElement('button');\n",
    "            capture.textContent = 'Capture Image';\n",
    "            div.appendChild(capture);\n",
    "\n",
    "            const video = document.createElement('video');\n",
    "            video.style.display = 'block';\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "            document.body.appendChild(div);\n",
    "            div.appendChild(video);\n",
    "            video.srcObject = stream;\n",
    "            await video.play();\n",
    "\n",
    "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "            return new Promise((resolve) => {\n",
    "                capture.onclick = () => {\n",
    "                    const canvas = document.createElement('canvas');\n",
    "                    canvas.width = video.videoWidth;\n",
    "                    canvas.height = video.videoHeight;\n",
    "                    canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "                    stream.getVideoTracks()[0].stop();\n",
    "                    div.remove();\n",
    "                    resolve(canvas.toDataURL('image/jpeg', quality));\n",
    "                };\n",
    "            });\n",
    "        }\n",
    "    ''')\n",
    "    display(js)\n",
    "    data = eval_js('takePhoto({})'.format(quality))\n",
    "    binary = b64decode(data.split(',')[1])\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(binary)\n",
    "    return filename\n",
    "\n",
    "\n",
    "# -- CALL THE RECOGNITION FUNCTION ONLY IF MODELS ARE LOADED --\n",
    "try:\n",
    "    print('Attempting to load classifier and scaler...')\n",
    "    knn_classifier = joblib.load(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    print('Classifier and scaler successfully loaded.')\n",
    "\n",
    "    # If loading is successful, proceed with live recognition\n",
    "    print(\"\\nStarting live face recognition...\")\n",
    "    recognize_live_face(knn_classifier, scaler, mtcnn, resnet, device)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f'\\nError: Classifier or scaler file not found. Please ensure you have completed the model training steps ({model_path}, {scaler_path}).')\n",
    "except Exception as e:\n",
    "    print(f'\\nError loading classifier or scaler: {e}')\n",
    "\n",
    "print(\"\\nLive recognition process finished (either completed or loading failed).\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
